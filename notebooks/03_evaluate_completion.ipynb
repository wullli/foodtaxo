{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202ba3f-7f9b-44d3-9c60-a620d9f2e319",
   "metadata": {},
   "source": "## Evaluate Taxonomy Completion"
  },
  {
   "cell_type": "code",
   "id": "62094114-0124-44b1-a2f8-b0703e4f9930",
   "metadata": {
    "tags": []
   },
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from llm_food_taxonomy.evaluation.metric import ScoreAccumulator\n",
    "from llm_food_taxonomy.evaluation.supervised.parent_metric import ParentMetric\n",
    "sys.path.append(\"..\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90590d82-c464-4267-a9fb-f74a27739f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "from llm_food_taxonomy.evaluation import PositionMetric, WuPSimilarity\n",
    "from llm_food_taxonomy.data.loader import load_taxonomy, load_completion\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60bf5fb6-dde5-4591-9de8-b120d2fc7217",
   "metadata": {
    "tags": []
   },
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:,.4f}'.format)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f3db40c7fae4891",
   "metadata": {},
   "source": [
    "trial = False\n",
    "mode = \"test\"\n",
    "dataset = \"bettybossi\"\n",
    "data_path = Path(f\"../data/{dataset}\")\n",
    "results_path = Path(f\"../output/{dataset}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "701fa38729cca2ab",
   "metadata": {},
   "source": [
    "terms, taxo = load_taxonomy(str(data_path), with_split=True)\n",
    "id_to_name = {d[\"node_id\"]: d[\"node_name\"] for d in terms.to_dict(orient=\"records\")}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5519676f43c2ddf1",
   "metadata": {},
   "source": [
    "terms.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f3125c456df965",
   "metadata": {},
   "source": [
    "taxo.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cf5d3104252f757",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "nodes_to_add = terms[terms.split == mode]\n",
    "train_nodes = terms[terms.split == \"train\"].node_id.apply(str)\n",
    "if trial:\n",
    "    nodes_to_add = nodes_to_add.node_id.iloc[:20].values.tolist()\n",
    "else:\n",
    "    nodes_to_add = nodes_to_add.node_id.values.tolist()\n",
    "nodes_to_add = [id_to_name[n] for n in nodes_to_add]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6565905aedaa7b91",
   "metadata": {},
   "source": [
    "seed_taxonomy_df = taxo[\n",
    "    taxo.apply(lambda r: (str(r.hypernym) in train_nodes.values) and (str(r.hyponym) in train_nodes.values), axis=1)]\n",
    "seed_taxonomy_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec0f9a1cdd5698f7",
   "metadata": {},
   "source": [
    "seed_taxonomy = seed_taxonomy_df.values.tolist()\n",
    "seed_taxonomy = [tuple(r) for r in seed_taxonomy]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90550e917a8e203b",
   "metadata": {},
   "source": [
    "[t for t in seed_taxonomy_df if t[0] == \"food\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29951afa327abe4f",
   "metadata": {},
   "source": [
    "len(nodes_to_add)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filter_words = []\n",
    "filter_words = [\"zero\"]\n",
    "valid_words = []\n",
    "# valid_words = [\"few_basic\"] #[\"Llama-3\"]"
   ],
   "id": "fbc98218acf31b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "879bed7c4ea2629d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "models_outs = {}\n",
    "\n",
    "outputs = [d for d in results_path.iterdir() if d.is_dir]\n",
    "\n",
    "for o in outputs:\n",
    "    try:\n",
    "        try:\n",
    "            model_name = f\"{o.split('_')[2]}_{o.split('_')[3]}\"\n",
    "        except:\n",
    "            model_name = Path(o).name\n",
    "        add_model = (not any(f in model_name for f in filter_words) if len(filter_words) > 0 else any(w in model_name for w in valid_words)) or (len(valid_words) == 0 and len(filter_words) == 0)\n",
    "        if add_model :\n",
    "            pred_terms, pred_triplets = load_completion(o, with_reasoning=True)\n",
    "            models_outs[model_name] = pred_terms, pred_triplets\n",
    "            print(f\"Loading {o}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {o}: {e}\")\n",
    "        traceback.print_exc()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c69b2d6b6bbb686",
   "metadata": {},
   "source": [
    "terms.sample(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "models_outs.keys()",
   "id": "fe9b597adb158846",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a12c959db9e10e3c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "nodes_to_add"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "terms[terms.node_name == \"cling\"]",
   "id": "f8373244a6d53f10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# models_outs['few_basic_Meta-Llama-3-70B-Instruct_2024-08-30_04-35-16'][1]",
   "id": "5e0a1b4ec6222a20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for model, (pred_terms, pred_triplets) in models_outs.items():\n",
    "    print(f\"Model: {model}, Pred Terms: {len(pred_terms)}, Pred Triplets: {len(pred_triplets)}\")"
   ],
   "id": "f4564c52b8efbf63",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3065e77a5c0bb3bb",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "source": [
    "metrics = [WuPSimilarity(), PositionMetric()]\n",
    "res = []\n",
    "pretty_names = {\"tacoprompt\": \"TacoPrompt\",\n",
    "                \"tmn\": \"TMN\",\n",
    "                \"arborist\": \"Arborist\",\n",
    "                \"temp\": \"TEMP\",\n",
    "                \"qen\": \"QEN\",\n",
    "                \"taxoexpan\": \"TaxoExpan\"}\n",
    "metric_cols = [\"WPS\", \"F1\", \"P\", \"R\", \"\"]\n",
    "nonleaf_cols = [f\"NL-{n}\" if n != \"\" else \"\" for n in metric_cols]\n",
    "leaf_cols = [f\"L-{n}\" if n != \"\" else \"\" for n in metric_cols][:-1]\n",
    "cols = [\"\", \"Model\"] + metric_cols + nonleaf_cols + leaf_cols\n",
    "preds = {}\n",
    "truths = {}\n",
    "populations = {}\n",
    "\n",
    "with tqdm(total=len(models_outs) * len(metrics), desc=\"Evaluating...\") as pbar:\n",
    "    for model, (pred_terms, pred_triplets) in models_outs.items():\n",
    "        pretty_model = pretty_names.get(model, model)\n",
    "        row = [\"\", pretty_model]\n",
    "        scores = []\n",
    "        nonleaf_scores = []\n",
    "        leaf_scores = []\n",
    "        populations[pretty_model] = {}\n",
    "        for m in metrics:\n",
    "            pred = {row.query_node: row.predicted_positions for _, row in pred_triplets.iterrows() if\n",
    "                    row.query_node in nodes_to_add}\n",
    "            truth = {row.node_name: row.positions for _, row in terms.iterrows() if row.node_name in nodes_to_add}\n",
    "            assert len(set(nodes_to_add) - set(truth.keys())) == 0, set(nodes_to_add) - set(truth.keys())\n",
    "            preds[model] = pred\n",
    "            s, nleaf_s, leaf_s = m.calculate(\n",
    "                pred_positions=deepcopy(pred),\n",
    "                true_positions=deepcopy(truth),\n",
    "                node2name=deepcopy(id_to_name),\n",
    "                seed_taxonomy=deepcopy(seed_taxonomy),\n",
    "                leaves=terms[terms.leaf].node_name.values.tolist(),\n",
    "                verbose=True\n",
    "            )\n",
    "            populations[pretty_model][type(m).__name__] = {}\n",
    "            populations[pretty_model][type(m).__name__][\"all\"] = s.pop(\"scores\")\n",
    "            populations[pretty_model][type(m).__name__][\"nonleaf\"] = nleaf_s.pop(\"scores\")\n",
    "            populations[pretty_model][type(m).__name__][\"leaf\"] = leaf_s.pop(\"scores\")\n",
    "            scores.extend(s.values())\n",
    "            nonleaf_scores.extend(nleaf_s.values())\n",
    "            leaf_scores.extend(leaf_s.values())\n",
    "            pbar.update(1)\n",
    "        row += scores + [\"\"] + nonleaf_scores + [\"\"] + leaf_scores\n",
    "        res.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(res, columns=cols).sort_values(by=\"Model\", ascending=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0cfea2d2ac120d9",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "display(res_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3582d4d91fa46776",
   "metadata": {},
   "source": [
    "## Hypothesis testing "
   ]
  },
  {
   "cell_type": "code",
   "id": "c37fb5eed6b4bc8",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "from scipy.stats import permutation_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59e074f5548527e3",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "def test_metric(x, y, axis=-1, scorer=lambda acc: acc.f1(nan=True)):\n",
    "    if len(x.shape) <= 2:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "    x = x.swapaxes(0, 1)\n",
    "    y = y.swapaxes(0, 1)\n",
    "    xtp, xfp, xfn = tuple(np.sum(x, axis=axis))\n",
    "    ytp, yfp, yfn = tuple(np.sum(y, axis=axis))\n",
    "\n",
    "    def _f1(tp, fp, fn):\n",
    "        a = ScoreAccumulator()\n",
    "        a.tp += tp\n",
    "        a.fp += fp\n",
    "        a.fn += fn\n",
    "        return scorer(a)\n",
    "    \n",
    "    x_stat = np.array([_f1(tp, fp, fn) for tp, fp, fn in zip(xtp, xfp, xfn)])\n",
    "    y_stat = np.array([_f1(tp, fp, fn) for tp, fp, fn in zip(ytp, yfp, yfn)])\n",
    "    diff = np.squeeze(np.abs(x_stat - y_stat))\n",
    "    return diff"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c67f45b2ac9518c2",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "from functools import partial\n",
    "\n",
    "p_values = []\n",
    "type_prefixes = {\"all\": \"\", \"nonleaf\": \"NL-\", \"leaf\": \"L-\"}\n",
    "metric_map = {\n",
    "    \"F1\": partial(test_metric, scorer=lambda acc: acc.f1(nan=True)),\n",
    "    \"P\": partial(test_metric, scorer=lambda acc: acc.precision(nan=True)),\n",
    "    \"R\": partial(test_metric, scorer=lambda acc: acc.recall(nan=True)),\n",
    "    \"WPS\": lambda x, y: np.mean(x) - np.mean(y)\n",
    "}\n",
    "for model1 in tqdm(populations.keys()):\n",
    "    for model2 in populations.keys():\n",
    "        if model1 == model2:\n",
    "            continue\n",
    "        for metric_name in [\"WuPSimilarity\", \"PositionMetric\"]:\n",
    "            for node_type in [\"all\", \"nonleaf\", \"leaf\"]:\n",
    "                m1_scores = populations[model1][metric_name][node_type]\n",
    "                m2_scores = populations[model2][metric_name][node_type]\n",
    "                score_names =  [\"F1\", \"P\", \"R\"] if metric_name != \"WuPSimilarity\" else [\"WPS\"]\n",
    "                for score_name in score_names:\n",
    "                    p_val = permutation_test((m1_scores, m2_scores),\n",
    "                                             statistic=metric_map[score_name],\n",
    "                                             permutation_type=\"samples\",\n",
    "                                             random_state=123,\n",
    "                                             n_resamples=1000).pvalue\n",
    "                    p_values.append([model1, model2, f\"{type_prefixes[node_type]}{score_name}\", p_val])\n",
    "\n",
    "stat_tests = pd.DataFrame(p_values, columns=[\"Model1\", \"Model2\", \"Score Name\", \"P-Value\"])\n",
    "stat_tests.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1af54159f2f5b477",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "stat_tests[(stat_tests.Model1 == \"few_basic_meta-llama-3-70b-instruct_2024-06-12_16-38-57\") \n",
    "                                    & (stat_tests.Model2 == \"temp\") \n",
    "                                    & (stat_tests[\"Score Name\"] == \"F1\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92d3c9f4096f97dc",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "stat_tests[(stat_tests.Model1 == \"TacoPrompt\") \n",
    "                                    & (stat_tests.Model2 == \"zero_meta-llama-3-70b-instruct_2024-06-11_13-42-43\") \n",
    "                                    & (stat_tests[\"Score Name\"] == \"NL-F1\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "p_num_cols = [c for c in stat_tests.columns if str(stat_tests.dtypes.loc[c]) == \"float64\"]\n",
    "pval_sdf = stat_tests.style.format('{:.4f}', subset=p_num_cols)\n",
    "print(pval_sdf.hide(axis=\"index\").to_latex())"
   ],
   "id": "ccdc5e18c939ac60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e820558b1093e61b",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# apply some formatting for all numbers (optional)\n",
    "alpha = 0.05\n",
    "significance_test = True\n",
    "df = res_df[res_df.Model.apply(lambda x: \"gpt\" not in x)]\n",
    "cols = [c for c in df.columns if str(df.dtypes.loc[c]) == \"float64\"]\n",
    "df_s = df.style.format('{:.4f}', subset=cols)\n",
    "\n",
    "# loop through rows and find which column for each row has the highest value\n",
    "for c in cols:\n",
    "    if str(df.dtypes.loc[c]) == \"float64\":\n",
    "        if significance_test:\n",
    "            row1 = df[c].idxmax()\n",
    "            best_row = df.loc[row1]\n",
    "            best_rows = [row1]  \n",
    "            for other_idx, other_row in df.iterrows():\n",
    "                if other_row.Model != best_row.Model:\n",
    "                    p_vals = stat_tests[(stat_tests.Model1 == str(best_row.Model)) \n",
    "                                        & (stat_tests.Model2 == str(other_row.Model)) \n",
    "                                        & (stat_tests[\"Score Name\"] == c)]\n",
    "                    assert len(p_vals) == 1, f\"Found {len(p_vals)} p-values for {c} with models: {best_row.Model.lower()} and {other_row.Model.lower()}\"\n",
    "                    p_val = p_vals.iloc[0][\"P-Value\"]\n",
    "                    if p_val > alpha:\n",
    "                        best_rows.append(other_idx)\n",
    "            df_s = df_s.format(lambda x: \"\\\\underline{\" + f'{x:.4f}' + \"}\", subset=(best_rows, c))\n",
    "            df_s = df_s.format(lambda x: \"\\\\textbf{\" + f'{x:.4f}' + \"}\", subset=(row1, c))\n",
    "        else:\n",
    "            row1, row2 = df.index.values[df[c].argsort()[::-1]][:2]\n",
    "            df_s = df_s.format(lambda x: \"\\\\textbf{\" + f'{x:.4f}' + \"}\", subset=(row1, c))\n",
    "            df_s = df_s.format(lambda x: \"\\\\underline{\" + f'{x:.4f}' + \"}\", subset=(row2, c))\n",
    "\n",
    "print(df_s.hide(axis=\"index\").to_latex())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc90b420c98a95f6",
   "metadata": {},
   "source": [
    "## Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fb148626827cfc6",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "model1 = 'few_basic_Meta-Llama-3-70B-Instruct_2024-08-30_04-35-16'\n",
    "model2 = 'none_Meta-Llama-3-70B-Instruct_2024-08-28_16-52-16'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1edacf23eb265d8f",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "truth = {row.node_name: row.positions for _, row in terms.iterrows() if row.node_name in nodes_to_add}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d13a3950ac0cf1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "is_executing": true
   },
   "source": [
    "import numpy as np\n",
    "model = model1\n",
    "scores = defaultdict(list)\n",
    "\n",
    "for q, true_pos in truth.items():\n",
    "    true_pos = set(true_pos)\n",
    "    pred_pos = set(preds[model].get(q, []))\n",
    "    all_pos = true_pos.union(pred_pos)\n",
    "    \n",
    "    def get_score(all_pos, pred_pos, true_pos):\n",
    "        tp, fp, fn, tn = 0, 0, 0, 0\n",
    "        for pos in all_pos:\n",
    "            if pos in true_pos and pos in pred_pos:\n",
    "                tp += 1\n",
    "            elif pos in true_pos and pos not in pred_pos:\n",
    "                fn += 1\n",
    "            elif pos not in true_pos and pos in pred_pos:\n",
    "                fp += 1\n",
    "            else:\n",
    "                raise ValueError(\"Something went wrong!\")\n",
    "        p = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        r = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "        return p, r, f1\n",
    "    \n",
    "    p, r, f1 = get_score(all_pos, pred_pos, true_pos)\n",
    "    scores[\"node_name\"].append(q)\n",
    "    scores[\"p\"].append(p)\n",
    "    scores[\"r\"].append(r)\n",
    "    scores[\"f1\"].append(f1)\n",
    "\n",
    "    \n",
    "error_df = "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af302b9ecb782d2c",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "preds[model]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c98b938d9642ffa",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "truth"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Error Analysis\n",
    "TODO: Majid"
   ],
   "id": "9171502239541656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [],
   "id": "883adb28d5c51473",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
